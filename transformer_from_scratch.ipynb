{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import unicodedata\n",
    "import spacy\n",
    "\n",
    "import tensorflow as tf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from concurrent.futures import as_completed\n",
    "from itertools import repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from http://storage.googleapis.com/download.tensorflow.org/data/fra-eng.zip\n",
      "\u001b[1m3423204/3423204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1us/step\n",
      "/Users/anushangawimalasena/.keras/datasets/fra.txt\n"
     ]
    }
   ],
   "source": [
    "# download dataset provided by Anki: https://www.manythings.org/anki/\n",
    "text_file = tf.keras.utils.get_file(\n",
    "    fname=\"fra-eng.zip\",\n",
    "    origin=\"http://storage.googleapis.com/download.tensorflow.org/data/fra-eng.zip\",\n",
    "    extract=True,\n",
    ")\n",
    "# show where the file is located now\n",
    "text_file = pathlib.Path(text_file).parent / \"fra.txt\"\n",
    "print(text_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_en = spacy.load('en_core_web_sm')\n",
    "nlp_fr = spacy.load('fr_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('perhaps he will never become famous .', '[start] peut-être que il ne devenir jamais célèbre . [end]')\n",
      "('tom check the date .', '[start] tom vérifier le date . [end]')\n",
      "('you be not suppose to be smoke in here .', '[start] vous ne être pas censé fumer ici . [end]')\n",
      "('you do not have to explain .', '[start] taire ne être pas obliger de fournir un explication . [end]')\n",
      "('he live in a big house .', '[start] il voir dans un grand maison . [end]')\n"
     ]
    }
   ],
   "source": [
    "def normalize(line, nlp_en, nlp_fr):\n",
    "    \"\"\"Normalize a line of text and split into two at the tab character\"\"\"\n",
    "    line = unicodedata.normalize(\"NFKC\", line.strip().lower())\n",
    "    line = re.sub(r\"^([^ \\w])(?!\\s)\", r\"\\1 \", line)\n",
    "    line = re.sub(r\"(\\s[^ \\w])(?!\\s)\", r\"\\1 \", line)\n",
    "    line = re.sub(r\"(?!\\s)([^ \\w])$\", r\" \\1\", line)\n",
    "    line = re.sub(r\"(?!\\s)([^ \\w]\\s)\", r\" \\1\", line)\n",
    "    \n",
    "    eng, fra = line.split(\"\\t\")\n",
    "    \n",
    "    eng = nlp_en(eng)\n",
    "    fra = nlp_fr(fra)\n",
    "\n",
    "    eng = ' '.join([token.lemma_ for token in eng])\n",
    "    fra = ' '.join([token.lemma_ for token in fra])\n",
    "\n",
    "    fra = \"[start] \" + fra + \" [end]\"\n",
    "    return eng, fra\n",
    "\n",
    "# Slow\n",
    "# # normalize each line and separate into English and French\n",
    "# with open(text_file) as fp:\n",
    "#     text_pairs = [normalize(line, nlp_en, nlp_fr) for line in fp]\n",
    "\n",
    "# # print some samples\n",
    "# for _ in range(5):\n",
    "#     print(random.choice(text_pairs))\n",
    " \n",
    "# with open(\"text_pairs_2.pickle\", \"wb\") as fp:\n",
    "#     pickle.dump(text_pairs, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize each line and separate into English and French\n",
    "with open(text_file) as fp:\n",
    "    text_input_lines = [line for line in fp]\n",
    "    len_rep = len(text_input_lines)\n",
    "    with ThreadPoolExecutor(max_workers = 10) as executor:\n",
    "        text_pairs = executor.map(normalize, text_input_lines, repeat(nlp_en, len_rep), repeat(nlp_fr, len_rep))\n",
    "\n",
    "# print some samples\n",
    "for _ in range(5):\n",
    "    print(random.choice(text_pairs))\n",
    " \n",
    "with open(\"text_pairs_2.pickle\", \"wb\") as fp:\n",
    "    pickle.dump(text_pairs, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"text_pairs.pickle\", \"rb\") as fp:\n",
    "    text_pairs = pickle.load(fp)\n",
    "\n",
    "# count tokens\n",
    "eng_tokens, fra_tokens = set(), set()\n",
    "eng_maxlen, fra_maxlen = 0, 0\n",
    "for eng, fra in text_pairs:\n",
    "    eng_tok, fra_tok = eng.split(), fra.split()\n",
    "    eng_maxlen = max(eng_maxlen, len(eng_tok))\n",
    "    fra_maxlen = max(fra_maxlen, len(fra_tok))\n",
    "    eng_tokens.update(eng_tok)\n",
    "    fra_tokens.update(fra_tok)\n",
    "print(f\"Total English tokens: {len(eng_tokens)}\")\n",
    "print(f\"Total French tokens: {len(fra_tokens)}\")\n",
    "print(f\"Max English length: {eng_maxlen}\")\n",
    "print(f\"Max French length: {fra_maxlen}\")\n",
    "print(f\"{len(text_pairs)} total pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "# Load normalized sentence pairs\n",
    "with open(\"text_pairs.pickle\", \"rb\") as fp:\n",
    "    text_pairs = pickle.load(fp)\n",
    "\n",
    "# train-test-val split of randomized sentence pairs\n",
    "random.shuffle(text_pairs)\n",
    "n_val = int(0.15*len(text_pairs))\n",
    "n_train = len(text_pairs) - 2*n_val\n",
    "train_pairs = text_pairs[:n_train]\n",
    "val_pairs = text_pairs[n_train:n_train+n_val]\n",
    "test_pairs = text_pairs[n_train+n_val:]\n",
    "\n",
    "# Parameter determined after analyzing the input data\n",
    "vocab_size_en = 10000\n",
    "vocab_size_fr = 20000\n",
    "seq_length = 20\n",
    "\n",
    "# Create vectorizer\n",
    "eng_vectorizer = TextVectorization(\n",
    "    max_tokens=vocab_size_en,\n",
    "    standardize=None,\n",
    "    split=\"whitespace\",\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=seq_length,\n",
    ")\n",
    "fra_vectorizer = TextVectorization(\n",
    "    max_tokens=vocab_size_fr,\n",
    "    standardize=None,\n",
    "    split=\"whitespace\",\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=seq_length + 1\n",
    ")\n",
    "\n",
    "# train the vectorization layer using training dataset\n",
    "train_eng_texts = [pair[0] for pair in train_pairs]\n",
    "train_fra_texts = [pair[1] for pair in train_pairs]\n",
    "eng_vectorizer.adapt(train_eng_texts)\n",
    "fra_vectorizer.adapt(train_fra_texts)\n",
    "\n",
    "# save for subsequent steps\n",
    "with open(\"vectorize.pickle\", \"wb\") as fp:\n",
    "    data = {\n",
    "        \"train\": train_pairs,\n",
    "        \"val\":   val_pairs,\n",
    "        \"test\":  test_pairs,\n",
    "        \"engvec_config\":  eng_vectorizer.get_config(),\n",
    "        \"engvec_weights\": eng_vectorizer.get_weights(),\n",
    "        \"fravec_config\":  fra_vectorizer.get_config(),\n",
    "        \"fravec_weights\": fra_vectorizer.get_weights(),\n",
    "    }\n",
    "    pickle.dump(data, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env--GFVdXs0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
